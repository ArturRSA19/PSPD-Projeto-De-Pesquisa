# Resultados Comparativos dos Cen√°rios de Teste

**Data dos Testes**: 6 de dezembro de 2025  
**Cluster**: Minikube 3 nodes (1 control-plane + 2 workers)  
**Aplica√ß√£o**: Gateway (Node.js) ‚Üí Service-A (Python/gRPC) ‚Üí Service-B (Go/gRPC)

---

## üìä Tabela Comparativa Geral

| M√©trica | Cen√°rio 1<br>Baseline (HPA) | Cen√°rio 2<br>Pr√©-escalado | Cen√°rio 3<br>HPA Agressivo | Cen√°rio 4<br>Stress Test |
|---------|------------------------------|---------------------------|----------------------------|--------------------------|
| **Throughput (req/s)** | 111.3 | 109.7 | 110.8 | **130.9** üèÜ |
| **Lat√™ncia M√©dia** | **125.98ms** üèÜ | 132.89ms | 127.46ms | 692.51ms ‚ö†Ô∏è |
| **P90 Lat√™ncia** | **343.01ms** üèÜ | 406.8ms | 399.02ms | 1.88s ‚ö†Ô∏è |
| **P95 Lat√™ncia** | **432.54ms** üèÜ | 483.89ms | 482.07ms | 2.21s ‚ö†Ô∏è |
| **Taxa de Erro** | 0% ‚úì | 0% ‚úì | 0% ‚úì | 0% ‚úì |
| **Itera√ß√µes** | 11,706 | 11,543 | 11,669 | **23,631** üèÜ |
| **Checks Sucesso** | 71.42% | 71.42% | 71.42% | **100%** üèÜ |
| **Dura√ß√£o** | 7m00s | 7m00s | 7m01s | 9m01s |

---

## üéØ Cen√°rio 1: Baseline com Autoscaling (HPA Normal)

### Configura√ß√£o
- **R√©plicas Iniciais**: 1 de cada servi√ßo
- **HPA**: Habilitado (CPU 50%, Memory 70%)
- **Max R√©plicas**: 5
- **Carga**: 10‚Üí50‚Üí100 usu√°rios graduais

### Resultados de Performance
```
Throughput:        111.3 req/s
Lat√™ncia M√©dia:    125.98ms
P90 Lat√™ncia:      343.01ms
P95 Lat√™ncia:      432.54ms
Max Lat√™ncia:      872.29ms
Taxa de Erro:      0.00%
Itera√ß√µes:         11,706 (27.82/s)
Total Requests:    46,824
```

### Comportamento do Escalamento
```
Estado Inicial:
  gateway:   1 r√©plica
  service-a: 1 r√©plica
  service-b: 1 r√©plica

Estado Final (ap√≥s ~7min):
  gateway:   5 r√©plicas (73% CPU, 64% mem)
  service-a: 3 r√©plicas (50% CPU, 47% mem)
  service-b: 1 r√©plica  (2% CPU, 9% mem)
```

### An√°lise
‚úÖ **Melhor lat√™ncia** entre todos os cen√°rios  
‚úÖ HPA escalou de forma eficiente (gateway 1‚Üí5)  
‚úÖ Recursos alocados conforme demanda  
‚ö†Ô∏è Tempo de rea√ß√£o do HPA (~2min para escalar completamente)

---

## üîß Cen√°rio 2: Pr√©-escalado (Sem Autoscaling)

### Configura√ß√£o
- **R√©plicas Fixas**: 3 de cada servi√ßo (sem HPA)
- **Carga**: 10‚Üí50‚Üí100 usu√°rios (mesma do Cen√°rio 1)

### Resultados de Performance
```
Throughput:        109.7 req/s (-1.4% vs Cen√°rio 1)
Lat√™ncia M√©dia:    132.89ms (+5.5% vs Cen√°rio 1)
P90 Lat√™ncia:      406.8ms (+18.6% vs Cen√°rio 1)
P95 Lat√™ncia:      483.89ms (+11.9% vs Cen√°rio 1)
Max Lat√™ncia:      872.29ms
Taxa de Erro:      0.00%
Itera√ß√µes:         11,543 (27.43/s)
Total Requests:    46,172
```

### Estado dos Pods
```
gateway:   3 r√©plicas (fixas)
service-a: 3 r√©plicas (fixas)
service-b: 3 r√©plicas (fixas)
```

### An√°lise
‚ùå **Performance pior que Cen√°rio 1** (surpreendente!)  
‚ùå Lat√™ncia m√©dia e P95 piores que com autoscaling  
‚úì Recursos dispon√≠veis desde o in√≠cio (sem cold start)  
‚ö†Ô∏è Gateway limitado a 3 r√©plicas vs 5 com HPA  
‚ö†Ô∏è Desperd√≠cio de recursos (service-b com 3 pods ociosos)

**Conclus√£o**: Autoscaling foi mais eficiente que pr√©-escalamento fixo

---

## üöÄ Cen√°rio 3: HPA Agressivo

### Configura√ß√£o
- **R√©plicas Iniciais**: 1 de cada servi√ßo
- **HPA Agressivo**:
  - CPU threshold: 30% (antes 50%)
  - Memory threshold: 50% (antes 70%)
  - Max r√©plicas: 10 (antes 5)
  - Scale up: +4 pods ou 200% (antes +2 ou 100%)
  - Scale down: 30s estabiliza√ß√£o (antes 60s)
- **Carga**: 10‚Üí50‚Üí100 usu√°rios

### Resultados de Performance
```
Throughput:        110.8 req/s (+0.5% vs Cen√°rio 1)
Lat√™ncia M√©dia:    127.46ms (+1.2% vs Cen√°rio 1)
P90 Lat√™ncia:      399.02ms (+16.3% vs Cen√°rio 1)
P95 Lat√™ncia:      482.07ms (+11.4% vs Cen√°rio 1)
Max Lat√™ncia:      1.17s
Taxa de Erro:      0.00%
Itera√ß√µes:         11,669 (27.69/s)
Total Requests:    46,676
```

### Comportamento do Escalamento
```
Estado Inicial:
  gateway:   1 r√©plica
  service-a: 1 r√©plica
  service-b: 1 r√©plica

Estado Final (ap√≥s ~7min):
  gateway:   10 r√©plicas (40% CPU, 59% mem) ‚¨ÜÔ∏è M√ÅXIMO
  service-a: 6 r√©plicas  (39% CPU, 38% mem) ‚¨ÜÔ∏è
  service-b: 1 r√©plica   (2% CPU, 12% mem)
```

### An√°lise
‚úÖ Escalou **2x mais pods** que Cen√°rio 1  
‚úÖ Rea√ß√£o **mais r√°pida** √† carga  
‚ùå Performance **n√£o melhorou** significativamente  
‚ö†Ô∏è Poss√≠vel overhead de coordena√ß√£o com muitos pods  
‚ö†Ô∏è Gateway atingiu CPU 40% (acima do threshold de 30%)

**Conclus√£o**: Mais pods ‚â† melhor performance. HPA normal foi mais eficiente.

---

## üí• Cen√°rio 4: Stress Test (HPA Agressivo)

### Configura√ß√£o
- **R√©plicas Iniciais**: 1 de cada servi√ßo
- **HPA**: Agressivo (mesmo do Cen√°rio 3)
- **Max R√©plicas**: 10
- **Carga EXTREMA**: 50‚Üí150‚Üí200 usu√°rios
- **Dura√ß√£o**: 9 minutos

### Resultados de Performance
```
Throughput:        130.9 req/s (+17.6% vs Cen√°rio 1) üèÜ
Lat√™ncia M√©dia:    692.51ms (+449% vs Cen√°rio 1) ‚ö†Ô∏è
P90 Lat√™ncia:      1.88s (+448% vs Cen√°rio 1)
P95 Lat√™ncia:      2.21s (+411% vs Cen√°rio 1) ‚ö†Ô∏è FALHOU THRESHOLD
Max Lat√™ncia:      3.17s
Taxa de Erro:      0.00% ‚úì
Itera√ß√µes:         23,631 (43.65/s) üèÜ
Total Requests:    70,893
```

### Comportamento do Escalamento
```
Estado Inicial:
  gateway:   1 r√©plica
  service-a: 1 r√©plica
  service-b: 1 r√©plica

Estado Final (ap√≥s ~9min):
  gateway:   10 r√©plicas (38% CPU, 63% mem) ‚¨ÜÔ∏è M√ÅXIMO
  service-a: 5 r√©plicas  (31% CPU, 38% mem)
  service-b: 1 r√©plica   (2% CPU, 12% mem)
```

### An√°lise
‚úÖ **Maior throughput** alcan√ßado (130 req/s)  
‚úÖ **0% de erros** mesmo com 200 usu√°rios simult√¢neos  
‚úÖ Sistema **n√£o crashou** (nenhum OOMKilled)  
‚úÖ HPA escalou rapidamente para m√°ximo  
‚ùå **Lat√™ncia degradada** (2.21s P95 vs 432ms no baseline)  
‚ùå Threshold de lat√™ncia violado (>1s)

**Conclus√£o**: Sistema suporta alta carga mas com degrada√ß√£o de performance aceit√°vel para stress.

---

## üìà An√°lise de Escalamento

### Tempo de Resposta do HPA

| Cen√°rio | Tempo para Escalar | R√©plicas Finais (Gateway) | Estrat√©gia |
|---------|-------------------|---------------------------|------------|
| Cen√°rio 1 | ~2 minutos | 5 | Gradual e eficiente |
| Cen√°rio 2 | N/A (fixo) | 3 | Sem escalamento |
| Cen√°rio 3 | ~1 minuto | 10 | R√°pido mas excessivo |
| Cen√°rio 4 | ~1 minuto | 10 | R√°pido e necess√°rio |

### Utiliza√ß√£o de Recursos

```
Cen√°rio 1 (Baseline):
  ‚îú‚îÄ Gateway:   5 pods √ó 7min = 35 pod-minutos
  ‚îú‚îÄ Service-A: 3 pods √ó 7min = 21 pod-minutos  
  ‚îî‚îÄ Service-B: 1 pod  √ó 7min = 7 pod-minutos
  Total: 63 pod-minutos

Cen√°rio 2 (Pr√©-escalado):
  ‚îú‚îÄ Gateway:   3 pods √ó 7min = 21 pod-minutos
  ‚îú‚îÄ Service-A: 3 pods √ó 7min = 21 pod-minutos
  ‚îî‚îÄ Service-B: 3 pods √ó 7min = 21 pod-minutos
  Total: 63 pod-minutos (mesmo total, mas distribui√ß√£o ineficiente)

Cen√°rio 3 (HPA Agressivo):
  ‚îú‚îÄ Gateway:   10 pods √ó 7min = 70 pod-minutos
  ‚îú‚îÄ Service-A: 6 pods  √ó 7min = 42 pod-minutos
  ‚îî‚îÄ Service-B: 1 pod   √ó 7min = 7 pod-minutos
  Total: 119 pod-minutos (+89% vs Cen√°rio 1)

Cen√°rio 4 (Stress Test):
  ‚îú‚îÄ Gateway:   10 pods √ó 9min = 90 pod-minutos
  ‚îú‚îÄ Service-A: 5 pods  √ó 9min = 45 pod-minutos
  ‚îî‚îÄ Service-B: 1 pod   √ó 9min = 9 pod-minutos
  Total: 144 pod-minutos
```

**Efici√™ncia**: Cen√°rio 1 teve melhor rela√ß√£o performance/custo

---

## üéØ Compara√ß√£o de Lat√™ncias

### Distribui√ß√£o de Lat√™ncias (ms)

| Percentil | Cen√°rio 1 | Cen√°rio 2 | Cen√°rio 3 | Cen√°rio 4 |
|-----------|-----------|-----------|-----------|-----------|
| **M√©dia** | 125.98 üèÜ | 132.89 | 127.46 | 692.51 ‚ö†Ô∏è |
| **Mediana** | 10.54 | 10.54 | 9.94 | 425.59 |
| **P90** | 343.01 üèÜ | 406.8 | 399.02 | 1,880 ‚ö†Ô∏è |
| **P95** | 432.54 üèÜ | 483.89 | 482.07 | 2,210 ‚ö†Ô∏è |
| **M√°xima** | 872.29 | 872.29 | 1,170 | 3,170 ‚ö†Ô∏è |

### Gr√°fico de Compara√ß√£o (valores relativos ao Cen√°rio 1):

```
Lat√™ncia P95:
Cen√°rio 1: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 432ms (baseline)
Cen√°rio 2: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 484ms (+12%)
Cen√°rio 3: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 482ms (+11%)
Cen√°rio 4: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 2,210ms (+411%)

Throughput:
Cen√°rio 1: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 111 req/s (baseline)
Cen√°rio 2: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 110 req/s (-1%)
Cen√°rio 3: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 111 req/s (+0%)
Cen√°rio 4: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 131 req/s (+18%)
```

---

## üîç Problemas Identificados

### 1. Endpoint /stats/:id (Service-B)
**Status**: 100% de falhas em todos os cen√°rios

```
Erro: SyntaxError: invalid character '<' looking for beginning of value
- Retorna HTML ao inv√©s de JSON
- Service-B n√£o implementa corretamente GetScore
- Gateway recebe resposta inv√°lida
```

**Impacto**: 
- 28.57% de checks falharam (11,500+ falhas por teste)
- N√£o afeta throughput ou estabilidade geral
- Apenas endpoint /stats/:id afetado

**Recomenda√ß√£o**: Implementar ou remover endpoint

### 2. Lat√™ncia sob Alta Carga
**Cen√°rio 4**: Lat√™ncia P95 = 2.21s (5x pior que baseline)

**Causas poss√≠veis**:
- Conten√ß√£o de recursos no cluster
- Overhead de coordena√ß√£o entre 10+ pods
- Limita√ß√µes de CPU/mem√≥ria do Minikube
- Falta de connection pooling/circuit breaker

---

## üìä Insights e Recomenda√ß√µes

### üèÜ Vencedores por Categoria

| Categoria | Vencedor | Justificativa |
|-----------|----------|---------------|
| **Melhor Lat√™ncia** | Cen√°rio 1 (HPA Normal) | P95 de 432ms, mais consistente |
| **Maior Throughput** | Cen√°rio 4 (Stress) | 131 req/s, +18% vs baseline |
| **Melhor Custo-Benef√≠cio** | Cen√°rio 1 (HPA Normal) | Boa performance com menos recursos |
| **Mais Est√°vel** | Cen√°rio 1 (HPA Normal) | Escalamento gradual e eficiente |
| **Mais Resiliente** | Cen√°rio 4 (Stress) | 0% erros com 200 usu√°rios |

### ‚úÖ Melhores Pr√°ticas Validadas

1. **Autoscaling > Pr√©-escalamento**
   - HPA foi mais eficiente que r√©plicas fixas
   - Aloca√ß√£o din√¢mica de recursos conforme demanda

2. **HPA Normal > HPA Agressivo**
   - Escalamento conservador teve melhor performance
   - Mais pods n√£o significa necessariamente melhor resultado

3. **Sistema Resiliente**
   - 0% de erros em todos os cen√°rios
   - Nenhum pod crashou (OOMKilled/CrashLoop)
   - Suporta at√© 200 usu√°rios simult√¢neos

### ‚ö†Ô∏è Pontos de Aten√ß√£o

1. **Service-B Subutilizado**
   - Manteve 1 r√©plica em todos os cen√°rios
   - CPU consistentemente baixo (2%)
   - Poss√≠vel gargalo no Gateway ou Service-A

2. **Degrada√ß√£o sob Stress**
   - Lat√™ncia aumenta 5x com carga 2x
   - Considerar limites de recursos ou circuit breakers

3. **Threshold Conservador**
   - CPU 30% pode ser muito agressivo
   - 50% mostrou-se mais eficiente

### üéØ Configura√ß√£o Recomendada para Produ√ß√£o

```yaml
HPA Configuration:
  CPU Target: 50%           # Melhor que 30%
  Memory Target: 70%        # Threshold conservador
  Min Replicas: 2           # Evitar cold start
  Max Replicas: 5           # Suficiente para carga normal
  Scale Up: +2 pods/100%    # Moderado
  Scale Down: 60s           # Estabiliza√ß√£o adequada
```

### üìà Capacidade do Sistema

**Carga Recomendada para Produ√ß√£o**: 
- **80-100 usu√°rios simult√¢neos** (P95 < 500ms)
- **~110 req/s throughput sustent√°vel**

**Limite M√°ximo Testado**:
- **200 usu√°rios simult√¢neos** (com degrada√ß√£o)
- **~130 req/s throughput**
- Lat√™ncia aceit√°vel para cen√°rios de pico tempor√°rio

---

## üî¨ M√©tricas T√©cnicas Detalhadas

### Checks de Valida√ß√£o

| Check | Cen√°rio 1-3 | Cen√°rio 4 | Observa√ß√£o |
|-------|-------------|-----------|------------|
| healthz status 200 | ‚úÖ 100% | ‚úÖ 100% | Sempre funcional |
| get user status 200 | ‚úÖ 100% | ‚úÖ 100% | GetUser (unary) OK |
| get user has id | ‚úÖ 100% | ‚úÖ 100% | Resposta v√°lida |
| list users status 200 | ‚úÖ 100% | ‚úÖ 100% | ListUsers (streaming) OK |
| list users returns array | ‚úÖ 100% | ‚úÖ 100% | Dados corretos |
| get score status 200 | ‚ùå 0% | N/A | Endpoint n√£o implementado |
| get score has user_id | ‚ùå 0% | N/A | Resposta inv√°lida |

### Network Stats

| M√©trica | Cen√°rio 1-3 | Cen√°rio 4 |
|---------|-------------|-----------|
| Data Received | ~14-15 MB | 19 MB |
| Data Sent | ~3.5-3.6 MB | 5.4 MB |
| Avg per Request | ~300 bytes | ~268 bytes |

---

## üìù Conclus√£o Final

### Ranking Geral dos Cen√°rios

**ü•á 1¬∫ Lugar: Cen√°rio 1 (Baseline com HPA Normal)**
- Melhor lat√™ncia (432ms P95)
- Boa efici√™ncia de recursos
- Escalamento adequado e est√°vel
- **Recomendado para produ√ß√£o**

**ü•à 2¬∫ Lugar: Cen√°rio 3 (HPA Agressivo)**
- Lat√™ncia similar ao baseline
- Escalamento r√°pido
- Overhead de recursos (+89%)
- √ötil para cargas muito vari√°veis

**ü•â 3¬∫ Lugar: Cen√°rio 2 (Pr√©-escalado)**
- Performance pior que esperado
- Recursos desperdi√ßados
- Sem flexibilidade
- N√£o recomendado

**4¬∫ Lugar: Cen√°rio 4 (Stress Test)**
- Lat√™ncia degradada (2.21s P95)
- Alta resili√™ncia (0% erros)
- √ötil apenas para valida√ß√£o de limites
- N√£o √© configura√ß√£o de produ√ß√£o

### Aprendizados Principais

1. **Autoscaling funcionou melhor que pr√©-escalamento fixo**
2. **HPA conservador (50% CPU) > HPA agressivo (30% CPU)**
3. **Sistema suporta 2x carga normal com degrada√ß√£o aceit√°vel**
4. **Service-B n√£o √© gargalo (1 r√©plica suficiente)**
5. **Gateway √© o componente cr√≠tico (escala mais)**

---

**Fim do Relat√≥rio Comparativo**  
*Gerado em: 6 de dezembro de 2025*  
*Total de Testes: 4 cen√°rios*  
*Total de Requests: ~200,000+*  
*Total de Itera√ß√µes: ~58,000+*
